<!doctype html>
<meta charset="utf-8" />

<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="https://robjhyndman.com/site_libs/clipboard/clipboard.min.js"></script>
<script src="https://robjhyndman.com/site_libs/bootstrap/bootstrap.min.js"></script>
<link href="https://robjhyndman.com/site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="https://robjhyndman.com/site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="https://fonts.googleapis.com/css?family=Fira+Sans|Merriweather" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hack-font@3/build/web/hack-subset.css">
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js", "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        TeX: {extensions: ["AMSmath.js","AMSsymbols.js",  "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"]},
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 1.2,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>

<style>
    /* Apply a max-width directly to the body and center it */
    body {
      max-width: 800px;    /* Limit the width */
      margin: 0 auto;      /* Center the content */
      padding: 20px;       /* Optional padding for better readability */
      background-color: #f9f9f9; /* Optional background color */
    }
</style>

</head>
<h1 id="using-conv2d-for-linear-projection-on-apple-neural-engine">Using
Conv2D for Linear Projection on Apple Neural Engine</h1>
<p>If you are interested in Apple Intelligence, you may want to check
out the 2022 blog post <a
href="https://machinelearning.apple.com/research/neural-engine-transformers">Deploying
Transformers on the Apple Neural Engine</a> and the more recent <a
href="https://machinelearning.apple.com/research/vision-transformers">Deploying
Attention-Based Vision Transformers to Apple Neural Engine</a>. Both
blog posts feature open sourced code that demonstrates the use of Conv2d
layers as replacements for linear projection:</p>
<ol type="1">
<li><a
href="https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/huggingface/distilbert.py">DistilBERT
implementation</a></li>
<li><a
href="https://github.com/apple/ml-vision-transformers-ane/blob/main/vision_transformers/attention_utils.py">Vision
Transformers implementation</a></li>
</ol>
<p>The 2022 blog post introduces this replacement as <em>Principle 1:
Picking the Right Data Format</em> of improving the Apple Neural Engine
(ANE) performance.</p>
<h2 id="empirical-verification">Empirical Verification</h2>
<p>The code snippet below demonstrates the interchangeability between
<code>nn.Conv2d</code> and <code>nn.Linear</code>. When projecting a
batch of <span class="math inline">\(B\)</span> <span
class="math inline">\(I\)</span>-dimensional vectors into the <span
class="math inline">\(O\)</span>-dimensional space, you can either:</p>
<ol type="1">
<li>use a traditional <span class="math inline">\(O\times I\)</span>
linear projection matrix, or</li>
<li>reshape the input batch to shape <span class="math inline">\((B, I,
1, 1)\)</span> and use a Conv2d layer with a kernel of shape <span
class="math inline">\((O, I, 1, 1)\)</span>.</li>
</ol>
<p>The output of the Conv2d operation will have the shape <span
class="math inline">\((B, O, 1, 1)\)</span>, which can be reshaped to
<span class="math inline">\((B, O)\)</span>, yielding results identical
to the linear projection. This equivalence holds whether or not a bias
term is included.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define batch size, input, and output dimensions</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>O <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create two layers with identical parameters.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> nn.Linear(in_features<span class="op">=</span>I, out_features<span class="op">=</span>O)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>I, out_channels<span class="op">=</span>O, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    conv.weight.data <span class="op">=</span> linear.weight.data.view(O, I, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    conv.bias.data <span class="op">=</span> linear.bias.data</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that both layers produce identical outputs.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B, I)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>linear_output <span class="op">=</span> linear(x)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>conv_output <span class="op">=</span> conv(x.view(B, I, <span class="dv">1</span>, <span class="dv">1</span>)).view(B, O)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(linear_output, conv_output)</span></code></pre></div>
<h2 id="how-conv2d-works">How Conv2d Works</h2>
<p>The simplest form of Conv2d outputs a grayscale image where each
pixel is the weighted average of an <span class="math inline">\(m\times
n\)</span> region in the input <span class="math inline">\(H\times
W\)</span> grayscale image. The <span class="math inline">\(m\times
n\)</span> weights are referred to as the kernel of the Conv2d
operation. If <span class="math inline">\(m=n=1\)</span>, the kernel
contains only a single scalar weight, and the Conv2d operation
effectively scales each pixel of the input image by this weight.</p>
<p>In the generalized form of Conv2d, input images can have multiple
channels (e.g., <span class="math inline">\(I\)</span> channels). For
instance, an image may have three channels for red, green, and blue.
Convolution over a multi-channel image requires a separate kernel for
each channel. Specifically:</p>
<ol type="1">
<li>Each input channel is convolved with its corresponding kernel,
resulting in one output image per channel.</li>
<li>The <span class="math inline">\(I\)</span>-channel outputs are then
summed to produce a single output channel.</li>
</ol>
<p>If the output is also multi-channel (e.g., <span
class="math inline">\(O\)</span> channels), the Conv2d operation
requires <span class="math inline">\(O\)</span> groups of <span
class="math inline">\(I\)</span>-channel kernels. Each kernel group
processes the <span class="math inline">\(I\)</span>-channel input
independently, and the results are concatenated into an <span
class="math inline">\(O\)</span>-channel output.</p>
<h2 id="conv2d-as-linear-projection">Conv2d As Linear Projection</h2>
<p>To linearly project an <span
class="math inline">\(I\)</span>-dimensional input vector into an <span
class="math inline">\(O\)</span>-dimensional space using Conv2d, we can
reinterpret the vector as an <span
class="math inline">\(I\)</span>-channel <span
class="math inline">\(1\times 1\)</span> image:</p>
<ol type="1">
<li>The <span class="math inline">\(O\times I\)</span> projection matrix
is represented as <span class="math inline">\(O\)</span> groups of <span
class="math inline">\(I\)</span>-channel <span
class="math inline">\(1\times 1\)</span> kernels.</li>
<li>The Conv2d operation applies these kernels to the input image,
producing an <span class="math inline">\(O\)</span>-channel <span
class="math inline">\(1\times 1\)</span> output image.</li>
</ol>
<p>When generalized to linear projection of a batch of <span
class="math inline">\(B\)</span> input vectors, we interprete the input
as <span class="math inline">\(B\)</span> <span
class="math inline">\(1\times 1\)</span> images, each with <span
class="math inline">\(I\)</span> channels. The output of Conv2d would be
a batch of <span class="math inline">\(B\)</span> <span
class="math inline">\(1\times 1\)</span> images, each with <span
class="math inline">\(O\)</span> channels. Then, the equivalence aligns
with the explanation in the previous section.</p>
<p>This approach bridges the gap between Conv2d and Linear, allowing the
efficient use of convolutional operations for tasks traditionally
handled by linear layers on Apple Neural Engine.</p>
