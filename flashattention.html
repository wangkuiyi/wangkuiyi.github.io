<!doctype html>
<meta charset="utf-8" />

<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="https://robjhyndman.com/site_libs/clipboard/clipboard.min.js"></script>
<script src="https://robjhyndman.com/site_libs/bootstrap/bootstrap.min.js"></script>
<link href="https://robjhyndman.com/site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="https://robjhyndman.com/site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="https://fonts.googleapis.com/css?family=Fira+Sans|Merriweather" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hack-font@3/build/web/hack-subset.css">
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js", "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        TeX: {extensions: ["AMSmath.js","AMSsymbols.js",  "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"]},
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 1.2,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>

<style>
    /* Apply a max-width directly to the body and center it */
    body {
      max-width: 800px;    /* Limit the width */
      margin: 0 auto;      /* Center the content */
      padding: 20px;       /* Optional padding for better readability */
      background-color: #f9f9f9; /* Optional background color */
    }
</style>

</head>
<h1 id="flashattention-part-3.">FlashAttention (Part 3.)</h1>
<p>The schematic illustration in the original paper is critical to
understand FlashAttention.</p>
<p><img src="flashattention.png" /img></p>
<p>Let us denote the index over the outter loop in the above figure as
<span class="math inline">\(i\)</span> and that for the inner loop as
<span class="math inline">\(k\)</span>. The operation of self-attention
is defined as:</p>
<p><span class="math display">\[O = \text{softmax}(Q K^T) V\]</span></p>
<p>Or, by separating the steps, we denote:</p>
<p><span class="math display">\[
X = Q K^T
A = \text{softmax}(X)
O = A V
\]</span></p>
<p>For a specific index <span class="math inline">\(k\)</span>, the
induction processes of self-attention is as the following. For the
simplicity, we ignore <span class="math inline">\(k\)</span>. Indeed,
each of the following variable, for example, <span
class="math inline">\(x_i\)</span>, should have been <span
class="math inline">\(x_{k,i}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
x_i &amp;= Q_k K_i^T \\
m_i &amp;= \max(m_{i-1}, x_i) \\
\delta_i &amp;= \delta_{i-1} \exp(m_{i-1}-m_i) + \exp(x_i-m_i) \\
a_i &amp;= \frac{\exp(x_i-m_N)}{\delta_N} \\
o_i &amp;= o_{i-1} + a_i V_i \\
\end{aligned}
\]</span></p>
<p>The first induction process comes from the fact that each element
<span class="math inline">\(a_{k,i}\)</span> is the result of a
dot-product between the <span class="math inline">\(k\)</span>-th row of
<span class="math inline">\(Q\)</span>, denoted by <span
class="math inline">\(Q_k\)</span>, and the <span
class="math inline">\(i\)</span>-th column of <span
class="math inline">\(K^T\)</span>, denoted as <span
class="math inline">\(K_i^T\)</span>.</p>
<p>The second, the third, and the fourth are from <a
href="online-softmax.html">the previous post about Online
Softmax</a>.</p>
<p>The last one is the induction process of the dot-product between the
<span class="math inline">\(k\)</span>-th row of <span
class="math inline">\(A\)</span>, denoted by <span
class="math inline">\(a_{k,i}\)</span>, or, short for <span
class="math inline">\(a_i\)</span>, and the <span
class="math inline">\(k\)</span>-th column of <span
class="math inline">\(V\)</span>, denoted as <span
class="math inline">\(V_{k,i}\)</span> and short for <span
class="math inline">\(V_i\)</span>.</p>
<p>Because the fourth induction rule depends on <span
class="math inline">\(m_N\)</span> and <span
class="math inline">\(\delta_N\)</span>, we counldnâ€™t start it before
the completion of the first three. However, the fourth and the fifth
could run in parallel.</p>
<p>A key contribution of FlashAttention is the derivation of a surrogate
<span class="math inline">\(\omega_i\)</span> to replace <span
class="math inline">\(a_i\)</span> and <span
class="math inline">\(o_i\)</span>. This <span
class="math inline">\(\omega_i\)</span> can run in parallel with the
first three induction rules.</p>
<p>As <span class="math inline">\(\omega_i\)</span> is a surrogate of
<span class="math inline">\(a_i\)</span> and <span
class="math inline">\(o_i\)</span>, let us examing <span
class="math inline">\(o_i\)</span> first.</p>
<p><span class="math display">\[
o_i = \sum_{j=1}^i \frac{\exp(x_j - m_N)}{\delta_N} V_j
\]</span></p>
<p>Using tricks explained in <a href="online-softmax.html">the post
about Online Softmax</a>, we want to define <span
class="math inline">\(\omega_i\)</span> in the following form so that
<span class="math inline">\(\omega_N=o_N\)</span>, which is the final
result that we want.</p>
<p><span class="math display">\[
\omega_i = \sum_{j=1}^i \frac{\exp(x_j - m_i)}{\delta_i} V_j
\]</span></p>
<p>Also, we want <span class="math inline">\(\omega_i\)</span> to be
inductive, so it should depends on <span
class="math inline">\(\omega_{i-1}\)</span>:</p>
<p><span class="math display">\[
\omega_{i-1} = \sum_{j=1}^i \frac{\exp(x_j - m_{i-1})}{\delta_{i-1}} V_j
\]</span></p>
<p>In order to rewrite <span class="math inline">\(\omega_i\)</span> as
a function of <span class="math inline">\(\omega_{i-1}\)</span>, we need
to move</p>
<p><span class="math display">\[
\begin{aligned}
\omega_i
&amp;= \sum_{j=1}^i \frac{\exp(x_j - m_i)}{\delta_i} V_j \\
&amp;= \sum_{j=1}^{i-1} \frac{\exp(x_j - m_i)}{\delta_i} V_j +
\frac{\exp(x_i - m_i)}{\delta_i} V_i \\
&amp;= \sum_{j=1}^{i-1} \frac{\exp(x_j - m_{i-1} + m_{i-1} -
m_i)}{\delta_{i-1}} \frac{\delta_{i-1}}{\delta_i} V_j + \frac{\exp(x_i -
m_i)}{\delta_i} V_i \\
&amp;= \sum_{j=1}^{i-1} \frac{\exp(x_j - m_{i-1})}{\delta_{i-1}}
\frac{\exp(m_{i-1} - m_i) \delta_{i-1}}{\delta_i} V_j + \frac{\exp(x_i -
m_i)}{\delta_i} V_i \\
&amp;= \omega_{i-1} \frac{\exp(m_{i-1} - m_i) \delta_{i-1}}{\delta_i} +
\frac{\exp(x_i - m_i)}{\delta_i} V_i \\
\end{aligned}
\]</span></p>
